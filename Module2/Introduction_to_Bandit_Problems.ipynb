{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Lab\n",
    "\n",
    "# Introduction to Bandit Problems\n",
    "\n",
    "Interest in **bandit problems** has a long history, starting in the 1940s. Bandit problems seem simple, but actually can be complex and hard to understand. There are a great number of bandit algorithms. In this lesson we will only look at the simplest case of an **exploring bandit algorithm**.   \n",
    "\n",
    "Bandit algorithms are surprisingly useful in practice. For example, in the past 10 years a number of researchers have incorporated variations on bandit algorithms into recommender systems. Bandit algorithms are used along with or as an alternative to matrix factorization methods. See for example the paper by [Louedec, et. al.](https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS15/paper/view/10385/10364).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Bandits?\n",
    "\n",
    "The name *Bandit* comes from the common name used for gambling slot machines. An example of a **one-armed** bandit in a casino is shown below.\n",
    "\n",
    "<img src=\"img/OneArmedBandit.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **A Physical One Armed Bandit** </center>  \n",
    "\n",
    " A player places a bet (inserts a token) into the bandit, and then pulls the lever. The player then receives a **reward**. The reward may be negative, the player has lost the amount bet. With some non-zero probability, a *lucky* player receives a positive reward and is considered to have won the game. It should come as no surprise that casinos set the probability of payout in a way that ensure **the house always wins** with high probability. \n",
    " \n",
    " This idea can be extended to a conceptual **multi-armed** bandit. The multi-armed bandit works in much the same way as the one armed bandit. The agent tries to optimize the reward by **learning** a **policy** of which lever is most likely to pay out. The cartoon below illustrates this idea.  \n",
    "\n",
    "<img src=\"img/multiarmedbandit.JPG\" alt=\"Drawing\" style=\"width:300px; height:200px\"/>\n",
    "<center> **A Conceptual Muti-Armed Bandit and an Agent Learning Optimal Policy**    \n",
    "Attribution: Microsoft Research</center>  \n",
    "\n",
    "The bandit problem is a simple version of a **reinforcement learning** problem:\n",
    "- The agent must learn the behavior of the environment by trial and error. The bandit is therefore model free. \n",
    "- The agent can take actions (pull a lever) in the environment. \n",
    "- The agent receives rewards (positive or negative) from the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit Model\n",
    "\n",
    "For this lesson we will use a simple bandit model. The agent received a positive reward of 1 with probability $p_k$ when pulling the kth lever. Otherwise the agent receives a reward of 0. We model this behavior as a series of **Bernoulli trials**. \n",
    "\n",
    "A **Bernoulli distribution** models the outcomes of multiple trials or experiments with binary outcomes. For example, pulling a lever results in two possible end states, $\\{ win:1,\\ loose:0 \\}$. For an event with a binary outcome, ${0,1}$ with probability $p$ of state 1, we can write the probability mass function for the Bernoulli distribution as:\n",
    "\n",
    "$$\n",
    "P(x\\ |\\ p) = \\bigg\\{ \n",
    "\\begin{matrix}\n",
    "p\\ if\\ x = 1\\\\\n",
    "(1 - p)\\ if\\ x = 0\n",
    "\\end{matrix}\\\\\n",
    "or\\\\\n",
    "P(x\\ |\\ p) = p^x(1 - p)^{(1-x)}\\ x \\in {0, 1}\n",
    "$$\n",
    "\n",
    "The agent uses a **policy**, $\\pi$, to determine which action to take. The expected **action value** given the action, $a$, by the policy is:   \n",
    "\n",
    "$$q_{\\pi}(a) = \\mathbb{E}_{\\pi} [R_{t}\\ |\\ A_t = a] $$\n",
    "\n",
    "Our goal is to find an **optimal policy** which maximizes the expected action value. We say that the optimal policy, $q_*(a)$, gives the highest expected value for the action $a$:\n",
    "\n",
    "$$q_{\\pi^*}(a) = \\mathbb{E}_{\\pi^*} [R_{t}\\ |\\ A_t = a] $$\n",
    "\n",
    "An optimal policy has an expected action value greater than or equal to all possible policies:\n",
    "\n",
    "$$q_{\\pi^*}(a) \\ge q_{\\pi}(a)\\ \\forall\\ \\pi$$\n",
    "\n",
    "The bandit model is stateless. Thus, there is no state required for the representation.\n",
    "\n",
    "For our multi-armed bandit, $p_k$ can be different for each lever. The agent must therefore try to find the optimal policy of which lever(s) to pull to maximize reward. There are two approaches an agent can take:  \n",
    "1. Pull each lever a few times and then estimate $p_k$ for each lever. The agent then adopts the policy of pulling the best lever(s) exclusively. We say that this algorithm **exploits** the best known policy. The problem is, that $p_k$ is only an estimate, and has a significant chance that the best lever is not selected from a finite number of pulls.   \n",
    "2. Alternatively, the agent can use an **exploring algorithm**. The exploring algorithm does the following:\n",
    "  -  With some small probability, $\\epsilon$, the agent takes an **exploring action** by pulling a lever at random. The estimate of $p_k$ for the lever pulled is then updated. It is possible a better policy can be discovered during this exploring step. \n",
    "  - With probability $1 - \\epsilon$ the agent exploits the best known policy. \n",
    "  \n",
    "The second algorithm is known as an **$\\epsilon$-greedy** method. This algorithm exhibits a key trade-off in reinforcement learning, between exploration to improve policy and exploitation of the best known policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit Agent Model\n",
    "\n",
    "The relationship between the **bandit agent** and the **environment** is illustrated in the figure below. \n",
    "\n",
    "<img src=\"img/BanditAgent.JPG\" alt=\"Drawing\" style=\"width:400px; height:300px\"/>\n",
    "<center> **Bandit Agent and Environment Model**</center>  \n",
    "\n",
    "The bandit agent is **model free**, which means that it has no information on the internal operation of the environment. The agent gains information on the environment by receiving **rewards** or utility information from the environment in response to **actions** the agent initiates in the environment. The environment informs the agent of the reward based on the agent's action, but without revealing the environment's underlying operation.   \n",
    "\n",
    "The bandit model is suitable for a **stateless** environment. Here the agent can pull any lever of the multi-armed bandit at any time. The environment (multi-armed bandit) has no state, and the expected reward for pulling a particular lever remains the same regardless of the sequence of events. \n",
    "\n",
    "The diagram above illustrates the key functions of the agent:\n",
    "- **Representation:** The representation in this case is simple. The agent maintains an estimate of the probability of success for each lever tried. In this particular example, we allow the agent to know the number of possible levers as part of its representation.  \n",
    "- **Learning:** The agent learns the probability of success based upon using the results of pulling levers. \n",
    "- **Inference:** The agent makes an **inference** to determine which lever to pull using the estimated probabilities of success in the representation. For **greedy** algorithms, the agent choses to pull the lever with the highest estimated probability of success. For $\\epsilon$-greedy algorithms, the agent's inference is to pull a random lever with probability $\\epsilon$ or choose the greedy action with probability $1-\\epsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Computational Example\n",
    "\n",
    "Let's try an example of a 10-armed bandit problem. The components we need for this example are:\n",
    "- **Environment simulator:** The arguments to this function are the actions the agent requests. In this case, the lever to pull and the number of pulls. \n",
    "- **Exploitation or greedy algorithm:** This algorithm **exploits** the current representation by continually pulling the lever with the highest estimated probability of success. The function makes multiple calls to the environment simulator and receives the rewards from pulling the best known lever.  \n",
    "- **Exploratory algorithm**. This algorithm uses the environment simulator to either exploit the lever with the best chance of success with with probability $1-\\epsilon$ or pulls a random lever with probability $\\epsilon$. The algorithm uses the reward received from the environment simulator to update its estimate of the lever with the highest probability of success. \n",
    "\n",
    "The code in the cell below imports the packages you will need. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment simulation\n",
    "\n",
    "The bandit agent is **model free** and therefore has no knowledge of the environment. The agent causes actions in the environment (pulls a lever) and receives rewards from the environment.     \n",
    "\n",
    "The code in the cell below defines an environment simulator function. The agent calls the simulator with the index of the lever to pull and the number of pulls. The simulator returns the success or failure from the pulls. The probabilities for each lever simulated are defined in a list. \n",
    "\n",
    "Execute this code to test the simulator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_levers = 10\n",
    "def simulate_pulls(lever, pulls = 1, probabilities = [0.24, 0.28, 0.32, 0.36, 0.40, 0.44, 0.48, 0.52, 0.56, 0.60]):\n",
    "    '''Function to simulate the environement given the\n",
    "    index of the lever to pull and the number of pulls'''\n",
    "    ## return the Binomially distributed result\n",
    "    return list(nr.binomial(n=1, p = probabilities[lever], size = pulls))\n",
    "\n",
    "[simulate_pulls(i, pulls = 10) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one should expect, the outcomes of each set of pulls are quite different. \n",
    "\n",
    "As a next step, let's examine the distribution of the $p_k$ values for a number of pulls on each lever. The function in the cell below calls the environment simulator a number of times for each of the levels. Execute the code to create a data frame of the realizations and examine the head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p(n_samples, n_pulls_sample, n_levers = n_levers):\n",
    "    '''Estimates the binomial probability, p, all 10 \n",
    "    bandit levers for n_samples with n_pulls_sample \n",
    "    samples in each set of pulls'''\n",
    "    samples = [simulate_pulls(i, pulls = n_pulls_sample) for i in range(n_levers)]\n",
    "    p = np.mean(samples, axis = 1)\n",
    "    for _ in range(n_samples - 1):\n",
    "        samples = [simulate_pulls(i, pulls = n_pulls_sample) for i in range(n_levers)]\n",
    "        p = np.concatenate((p, np.mean(samples, axis = 1))) #, axis = 1)\n",
    "    return pd.DataFrame(p.reshape(n_samples, n_levers))\n",
    "    \n",
    "nr.seed(445)\n",
    "estimates = compute_p(100, 100)    \n",
    "estimates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the head of this table, noticing that the values of $p_k$, for each lever, are quite different from experiment to experiment. To get a feel for this variability, execute the code in the cell below to display violin plots of the distribution of $p_k$ for each lever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.violinplot(data = estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this chart you can see the considerable variation in the estimated values of $p_k$. Still average the values of pulling each of the levers is considerably different. \n",
    "\n",
    "### Exploitation Algorithm\n",
    "\n",
    "As a next step, let's try a simple exploitation algorithm. In the cell below, a function to find a policy is defined. The values of $p_k$ are estimated based on 5 pulls of each lever. Policy is determined by choosing the lever with the maximum $p_k$. **Ties are broken** by arbitrarily selecting the first lever in a list of possibilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_lever(n_pulls, n_levers = n_levers):\n",
    "    vals = [simulate_pulls(i, pulls = n_pulls) for i in range(n_levers)]\n",
    "    p = np.mean(vals, axis = 0)\n",
    "    max_p = np.max(p)\n",
    "    return np.where(p == max_p)[0][0]\n",
    "\n",
    "nr.seed(6445)\n",
    "max_bandit = find_max_lever(n_pulls = 5)\n",
    "max_bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the **optimal policy** based on the limited sample. The code in the cell below defines a function which simulates 1,000 pulls of the lever determined to be optimal. The average accumulated reward for each time step is computed by dividing the cumulative sum of rewards by the number of time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploit(n_samps, max_bandit):\n",
    "    ## Find the reward for each of the pulls\n",
    "    samples = [simulate_pulls(max_bandit) for _ in range(n_samps)]\n",
    "    ## Find the cumulative reward and then normalize by the number of pulls\n",
    "    samples = np.cumsum(samples)\n",
    "    proportion = [s/float(i+1) for i,s in enumerate(samples)]\n",
    "    return proportion\n",
    "\n",
    "prob_success = exploit(10000, max_bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to plot the average reward accumulated at each time step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, the average reward for this exploiting approach converges to the value of $p_k$ for the lever chosen for the policy. This result highlights an important property of greedy algorithms. They have no way to improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploring Algorithm\n",
    "\n",
    "The function defined in the cell below implements a simple exploring algorithm. Policy is improved as the algorithm proceeds through the time steps. The algorithm works as follows:    \n",
    "1. The algorithm loops over a number of **episodes**. The mean of the accumulated rewards is computed over the results from the episodes. \n",
    "2. Within each episode an initial random policy is selected.\n",
    "3. An inner loop iterates over the number of time steps of samples in each episode\n",
    "4. With probability $\\epsilon$ a random exploration step is taken. If the policy can be improved the new policy is adopted. \n",
    "5. The average accumulated rewards are computed. To limit memory use, the following commonly used update  expression is applied, where the number of steps are the steps to that point:   \n",
    "$$NewEstimate \\leftarrow \\frac{OldEstimate * NumberOfSteps + NewReward}{NumberOfSteps + 1}$$\n",
    "\n",
    "Some Specific details of this code can be learned by reading the comments. \n",
    "\n",
    "You must complete the missing code in the inner loop. Then execute the code for 100 episodes of 10,000 samples per-episode using $\\epsilon = 0.01$. The execution may take some time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def explore(epsilon): return nr.uniform() < epsilon\n",
    "\n",
    "def update_prob_count(current_prob_count, bandit, sample):\n",
    "    '''Function does the bookkeeping of the probability \n",
    "    of success for each of the levers'''\n",
    "    current_prob_count[bandit,0] = (current_prob_count[bandit,1] * current_prob_count[bandit,0] + sample)/(current_prob_count[bandit,1] + 1)\n",
    "    current_prob_count[bandit,1] = current_prob_count[bandit,1] + 1\n",
    "    return current_prob_count\n",
    "    \n",
    "\n",
    "def exploit_explore(n_samps, epsilon, n_levers = n_levers):\n",
    "    '''Function to execute a the samples for an episodes'''\n",
    "    samples = []\n",
    "    ## Choose a random lever to start the episode\n",
    "    max_bandit = nr.choice(range(n_levers))\n",
    "    ## Initialize an array to do the bookkeeping to determine the best bandit\n",
    "    current_prob_count = np.zeros((n_levers,2))\n",
    "    for i in range(n_samps):\n",
    "        if explore(epsilon): \n",
    "            ## Exploring, so choose a random lever to pull\n",
    "            new_bandit= nr.choice(range(n_levers))\n",
    "            #### ADD MISSING CODE HERE ####\n",
    "            ## Pull the lever and update the stats.\n",
    "            samples.append(simulate_pulls(new_bandit))\n",
    "            current_prob_count = update_prob_count(current_prob_count, new_bandit, samples[-1])\n",
    "            #### ADD TWO LINES OF MISSING CODE HERE ####\n",
    "            ## Find the bandit with the largest probability of success\n",
    "            max_bandit = np.argmax(current_prob_count[:,0])\n",
    "        else: ## Or, exploit \n",
    "            ## Pull the lever and update the stats.\n",
    "            samples.append(simulate_pulls(max_bandit))\n",
    "            current_prob_count = update_prob_count(current_prob_count, max_bandit, samples[-1])   \n",
    "    return(np.array(samples).flatten())    \n",
    "\n",
    "def e_greedy(n_samps, episodes, epsilon):\n",
    "    results = np.zeros((episodes,n_samps))\n",
    "    for i in range(episodes): ## Iterate over the episodes\n",
    "        results[i,:] = exploit_explore(n_samps, epsilon)\n",
    "    ## Return the average return for at each time step    \n",
    "    return [s/(x + 1) for s,x in zip(np.cumsum(np.mean(results, axis = 0)), range(n_samps))]    \n",
    "\n",
    "nr.seed(33355)\n",
    "prob_success = e_greedy(10000, 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below print the average reward for steps 2,000, 4,000, 6,000, 8,000 and 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1999,3999,5999,7999,9999]:\n",
    "    print('Step = {}, average reward = {}'.format(i,prob_success[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to display a plot of the average reward by time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average reward slowly converges to the value of $p_k$ for the best lever. The policy is clearly being improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "\n",
    "Next, try another value of the exploitation-exploration trade-off parameter, $\\epsilon = 0.2$. In this case, every fifth step will be exploratory, whereas for the first example every 100th step was exploratory. The question is, how does this change performance of the algorithm?\n",
    "\n",
    "In the cell below create and execute the code to compute, print the average reward for steps 2,000, 4,000, 6,000, 8,000 and 10,000, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(7879)\n",
    "prob_success = e_greedy(10000, 100, 0.3)\n",
    "\n",
    "for i in [1999,3999,5999,7999,9999]:\n",
    "    print('Step = {}, average reward = {}'.format(i,prob_success[i]))\n",
    "\n",
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Greedy case \n",
    "\n",
    "Finally, it is worth checking how this algorithm behaves with  $\\epsilon = 0.0$, the pure exploitation case. Complete the code in the cell below and compute, print the average reward for steps 2,000, 4,000, 6,000, 8,000 and 10,000, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(7880)\n",
    "prob_success = e_greedy(10000, 100, 0.0)\n",
    "\n",
    "for i in [1999,3999,5999,7999,9999]:\n",
    "    print('Step = {}, average reward = {}'.format(i,prob_success[i]))\n",
    "\n",
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the result is a sub-optimal policy. Further, this policy cannot improve, since there is no exploration of alternatives. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
