{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Policy Gradients\n",
    "\n",
    "In the previous lesson we created parameterized state value and action value functions:\n",
    "\n",
    "$$V(s_t) \\approx  V(s_t,\\mathbf{w}_t) \\\\\n",
    "Q(s_t,a_t) \\approx Q(s_t,a_t,\\mathbf{w}_t)$$\n",
    "\n",
    "Where, $\\mathbf{w}$ is the parameter vector. Using these parameterized functions optimal policies are computed. \n",
    "\n",
    "Now, we will consider **parameterized policy functions** which can be written in the form:   \n",
    "\n",
    "$$\\pi(a\\ |\\ s, \\mathbf{\\theta}) = Pr\\{A_t = a\\ |\\ S_t = s, \\mathbf{\\theta_t} = \\mathbf{\\theta} \\}$$   \n",
    "\n",
    "Where, $\\mathbf{\\theta} \\in R^d$ is the d-dimensional **parameter vector**. The parameterized policy is often referred to as the **actor**.   \n",
    "\n",
    "A **parameterized value function**, $\\hat{v}(s, \\mathbf{w})$, can be used to evaluate a policy. The value function is determined by state, s, and the d-dimensional parameter vector $\\mathbf{w} \\in R^d$. The parameterized value function is often referred to as the **critic**.  \n",
    "\n",
    "\n",
    "**Actor-critic** algorithms use gradient methods to learn both the policy parameters and the value function or critic.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterized Policy Approximation\n",
    "\n",
    "How can we parameterize a policy?   \n",
    "\n",
    "First, the parameterized policy must be **differentiable** with respect to to the parameter vector, $\\mathbf{\\theta}$, to be amenable to gradient ascent methods. To be learnable the policy gradient with respect to $\\mathbf{\\theta} \\in R^d$, $\\nabla_{\\mathbf{\\theta}} \\pi(a\\ |\\ s, \\mathbf{\\theta})$, must exist and be bounded for $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$, where $\\mathcal{S}$ is the set of all states, and $\\mathcal{A}(s)$ is the set of all actions given s.  \n",
    "\n",
    "Second, the policy must never become deterministic. In other words, the probability of taking an action must not be simply binary, $\\{ 0,1 \\}$, or $\\pi(a\\ |\\ s, \\mathbf{\\theta}) \\in \\{ 0,1 \\}$. Instead, a viable policy must allow each possible action with some probability for all states,  $0 \\gt \\pi(a\\ |\\ s, \\mathbf{\\theta}) \\gt 1.0,\\ \\forall\\ a,\\ \\forall\\ s$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of parameterized policy \n",
    "\n",
    "You may be wondering what the advantages and disadvantages of parameterized policy might be? The advantages can be summarized as:\n",
    "\n",
    "- **Improved convergence properties**. In some cases, learning a parameterized policy can be more sample efficient than other RL learning methods.   \n",
    "- **Scalable to high dimensional and continuous action spaces**. We have investigated methods to learn policy for continuous state spaces. But, the examples we have examined to now have discrete action spaces. However, many real-world problems have continuous action spaces. Parameterized policy methods work well with continuous action spaces. \n",
    "- **Can learn a stochastic policies**. All of the algorithms we have examined until now create deterministic policies. Whereas, parameterized policy can be stochastic.   \n",
    "\n",
    "The disadvantages of a parameterized policy include:\n",
    "\n",
    "- These algorithms will **often converge to a locally optimal solutions**, rather than a globally optimal solutions.  \n",
    "- Policy evaluation is relatively inefficient and has high variance. We will examine methods to reduce the variance shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy optimization   \n",
    "\n",
    "Learning with parameterized policy involves maximizing the value of the policy. A number of optimization methods have been used for this problem. Here, we will use **gradient ascent** to maximize the loss function.      \n",
    "\n",
    "The goal of policy gradient methods is to learn a parameter vector, $\\mathbf{\\theta}$, which **maximizes a loss function**, $J(\\mathbf{\\theta})$. The commonly used learning method is to apply **gradient ascent** method of the form:  \n",
    "\n",
    "$$\\mathbf{\\theta}_{t+_1} = \\mathbf{\\theta}_t + \\alpha \\widehat{\\nabla J(\\mathbf{\\theta})}$$  \n",
    "\n",
    "Where,\n",
    "\n",
    "$\\alpha = $ the learning rate.  \n",
    "$\\widehat{\\nabla J(\\mathbf{\\theta})} \\in R^d = $ the estimate of the d-dimension **gradient** vector of the loss function:\n",
    "\n",
    "$$\\widehat{\\nabla_{\\theta} J(\\mathbf{\\theta})} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_d}\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Theorem  \n",
    "\n",
    "We can find the **policy gradient** analytically, if $\\pi_{theta}$ is **differentiable and non-zero everywhere**. The gradient is then $\\nabla_{\\theta} \\pi_{\\theta}$. But, how can this gradient be found in practice? The answer is to apply the **policy gradient theorem**.    \n",
    "\n",
    "For an episodic MDP we can define the performance by the loss function:\n",
    "\n",
    "$$J(\\mathbf{\\theta}) = v_{\\pi_{\\mathbf{\\theta}}}(s_0)$$  \n",
    "\n",
    "Where $s_0$ is the starting state of the episode. In this case, there is no discounting, with $\\gamma = 1$.\n",
    "\n",
    "Given the loss function defined above the policy gradient theorem says that the gradient is:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\\\ = \\mathbb{E}_{\\pi_\\theta} \\big[ \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) \\big]$$\n",
    "\n",
    "For a **one=step Markov Decision Process** (MDP) we can  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio and score function\n",
    "\n",
    "There is an efficient way to find the gradient of the policy $\\pi(a|S_t,\\mathbf{\\theta})$. An identity of **likelihood ratios** can be used:\n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\pi(a|S_t,\\mathbf{\\theta}) \\frac{ \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})}{\\pi(a|S_t,\\mathbf{\\theta}))} $$    \n",
    "\n",
    "Now, use the following identity:\n",
    "\n",
    "$$\\frac{ \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})}{\\pi(a|S_t,\\mathbf{\\theta}))} = \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})$$\n",
    "\n",
    "Where $\\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})$ is the **score function**.\n",
    "\n",
    "Substituting the score function gives: \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\pi(a|S_t,\\mathbf{\\theta}) \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Policies\n",
    "\n",
    "Algorithms we have examined previously, value iteration and policy iteration, result in **deterministic policies**. A deterministic policy takes an optimal action given the state. \n",
    "\n",
    "But, what happens if there is uncertainty as to the best action? In this case, a **stochastic policy** is required. As you likely intuit, the action taken by a stochastic policy is probabilistic.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete stochastic actions    \n",
    "\n",
    "The deterministic policies we have examined previously all take discrete actions. A policy with deterministic discrete actions can be represented, $\\pi(a|s) \\in {0,1}$. In other words, a binary response, an action is either taken or not. \n",
    "\n",
    "Alternatively, the actions taken by a stochastic policy are determined probabilistically. If there are a limited number of possible actions, the probability of taking an action can be computed as **softmax action preferences**:\n",
    "\n",
    "$$\\pi(a|s, \\mathbf{\\theta}) = \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_b e^{h(s,a,\\mathbf{\\theta})}}$$   \n",
    "\n",
    "The action preferences with the largest probabilities are the most likely to be taken.    \n",
    "\n",
    "For the case of policy parameterization using linear function approximation, $\\phi(s,a)\\ \\mathbf{\\theta}$:    \n",
    "\n",
    "$$\\pi(a|s, \\mathbf{\\theta}) \\propto  e^{\\phi(s,a)^T\\ \\mathbf{\\theta}}$$\n",
    "\n",
    "The score function then becomes:  \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\phi(s,a) - \\mathbb{E}_{\\pi_\\theta} \\big[ \\phi(s,\\cdot) \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous actions and Gaussian distributions   \n",
    "\n",
    "Many real world problems have continuous action spaces. Parameterized policies are ideal for continuous action spaces. A stochastic policy for a continuous action space can be parameterized using a Gaussian distribution:    \n",
    "\n",
    "$$a \\sim \\mathcal{N} \\big( \\mu(s),\\sigma^2 \\big)$$  \n",
    "\n",
    "where, the mean action is parameterized, $u(s) = \\pi(s)^T\\ \\mathbf{\\theta}$. It is also possible to parameterize $\\sigma^2$.\n",
    "\n",
    "The score function is then:   \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\frac{\\big(a - \\mu(s) \\big) \\pi(s)}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Gradient - Reinforce  \n",
    "\n",
    "By direct application of the policy gradient theorem the **reinforce algorithm** can be developed. For each episode, the steps of the reinforce algorithm are: \n",
    "\n",
    "1. Using Monte Carlo policy evaluation, the state value, $v(s)$, is computed.  \n",
    "2. Update the policy parameters using the policy gradient theorem:\n",
    "\n",
    "$$\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t + \\alpha\\ \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\mathbf{\\theta}})\\  v_t(s)$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.  \n",
    "\n",
    "While the reinforce algorithm converges, the variance of the Monte Carlo policy gradient can be large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Variance with a Critic\n",
    "\n",
    "How can the variance of the policy gradient be reduced? Can use a **critic** to estimate the action-value function:\n",
    "\n",
    "$$Q_{\\pi_{\\mathbf{\\theta}}}(s,a) \\approx Q_{w}(s,a)$$\n",
    "\n",
    "The steps of the actor-critic algorithm alternates between these steps:\n",
    "- **Critic** evaluates the current parameterized policy. The critic updates the action-value function parameters, $w$.\n",
    "- **Actor** determines the policy of actions. The actor updates the policy parameters, $\\theta$, using the critic update. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic with approximate policy gradient\n",
    "\n",
    "The actor-critic algorithm uses an **approximate policy gradient**.\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\approx \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a) \\big]$$  \n",
    "\n",
    "Which makes the parameter update:   \n",
    "\n",
    "$$\\Delta \\theta = \\alpha\\ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a)$$\n",
    "\n",
    "How to estimate the action-value, $Q_w(s,a)$? Can use the policy evaluation of $\\pi_\\theta$, for the parameters $\\theta$. We have examine several methods for policy evaluation:   \n",
    " \n",
    "- Monte Carlo policy evaluation.\n",
    "- Temporal difference (TD) policy evaluation. \n",
    "- Least squares fitting of policy evaluation function by least squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in Actor-Critic methods\n",
    "\n",
    "Using an approximate policy gradient introduces **bias**, which can lead to poor convergence of the solution. How can one choose a value function approximation which minimizes this bias. There are two criteria which must be met:\n",
    "\n",
    "First, the value function must be **compatible** with the policy. By this we mean the following relationship should be true:\n",
    "\n",
    "$$\\nabla_w\\ Q_w(s,a) = \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)$$\n",
    "\n",
    "Second, the value function must have parameters, $\\mathbf{w}$ which minimizes the mean squared error:   \n",
    "\n",
    "$$\\epsilon = \\mathbb{E}_{\\pi_\\theta} \\big[ \\big( Q_{\\pi_\\theta}(s,a) - Q_w(s,a) \\big)^2 \\big]$$\n",
    "\n",
    "The above criteria leads to the following exact policy gradient that meets both:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) = \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a) \\big]$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient for Critic\n",
    "\n",
    "How is the parameter vector, **w**, for the critic updated? As with the policy parameter vector, $\\theta$, the critic parameter vector is updated using gradient ascent. \n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\ \\delta_t\\  \\nabla_w\\ Q_w(s,a)$$\n",
    "\n",
    "where,  \n",
    "$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1},a) - Q(S_t,A_t)$ is the TD error,  \n",
    "$\\nabla_w\\ Q_w(s,a)$ is the gradient of $Q_w(s,a)$ with respect to **w**.\n",
    "\n",
    "How can the gradient, $\\nabla_w\\ Q_w(s,a)$,be computed. A simple case is to represent $Q(s,a)$ using a linear function approximation: \n",
    "\n",
    "$$Q_w(s,a) = \\phi(s,a)^T \\mathbf{w}$$  \n",
    "\n",
    "Where, $\\phi(s,a)$ are the basis functions. \n",
    "\n",
    "Since the $Q_w(s,a)$ is linear in **w**:\n",
    "\n",
    "$$\\nabla_w\\ Q_w(s,a) = \\phi(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, log\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_car(x, x_dot, acceleration, x_lims = (-1.2,0.5), x_dot_lims = (-0.07,0.07), a_lims = (-1.0,1.0)):\n",
    "    ## Check the limits on acceleration\n",
    "    if(acceleration < x_lims[0]): acceleration = -1.0\n",
    "    if(acceleration > x_lims[1]): acceleration = 1.0\n",
    "    \n",
    "    ## Compute velocity within limits\n",
    "    x_dot_prime = x_dot + 0.001 * acceleration - 0.0025 * cos(3.0 * x)\n",
    "    if(x_dot_prime < x_dot_lims[0]): x_dot_prime = x_dot_lims[0]\n",
    "    if(x_dot_prime > x_dot_lims[1]): x_dot_prime = x_dot_lims[1]\n",
    "        \n",
    "    ## Now update position\n",
    "    x_prime = x + x_dot\n",
    "    if(x_prime < x_lims[0]): x_prime = x_lims[0]\n",
    "    if(x_prime > x_lims[1]): x_prime = x_lims[1]\n",
    "      \n",
    "    ## At the terminal state or not and set reward\n",
    "    if(x_prime >= x_lims[1]): \n",
    "        done = True\n",
    "        reward = 100.0\n",
    "    else: \n",
    "        done = False\n",
    "        reward = -1.0\n",
    "        \n",
    "    return(x_prime, x_dot_prime, done, reward)    \n",
    "        \n",
    "def initalize_car(x_lims = (-0.6,-0.4)):\n",
    "    ## Find random start for car\n",
    "    return(nr.uniform(x_lims[0],x_lims[1]))\n",
    "\n",
    "## Test the function\n",
    "a = -0.0\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "for i in range(100):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "def plot_car(x, x_dot):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(212)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "    \n",
    "plot_car(x,x_dot)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_state(x, x_lims = (-1.2,0.5), n_tiles = 20):\n",
    "    \"\"\"Function to compute tile state given positon\"\"\"\n",
    "    state = int((x - x_lims[0])/(x_lims[1] - x_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-1.2,0.5,20)):\n",
    "    print('x = ' + str(x) + ' state = ' + str(x_state(x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_dot_state(x_dot, x_dot_lims = (-0.07,0.07), n_tiles = 20):\n",
    "    \"\"\"Function to compute tile state given velocity\"\"\"\n",
    "    state = int((x_dot - x_dot_lims[0])/(x_dot_lims[1] - x_dot_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-0.07,0.07,20)):\n",
    "    print('x_dot = ' + str(x) + ' state = ' + str(x_dot_state(x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q(x, x_dot, w):\n",
    "    '''Function to compute action value, Q, given state and parameter vector w'''\n",
    "    return w[0, x_state(x), x_dot_state(x_dot)] * x + w[1, x_state(x), x_dot_state(x_dot)] * x_dot\n",
    "\n",
    "w = np.array(2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_log_pi(a, x, x_dot, theta, sigma):\n",
    "    '''This function computes the gradients of the log probability\n",
    "    for the policy given the state and action. The function assumes\n",
    "    a stochastic Gaussina distributed action space'''\n",
    "    delta_pi = (a - theta[x_state(x), x_dot_state(x_dot)])/sigma\n",
    "    if(delta_pi > 0): delta_pi = log(delta_pi)\n",
    "    else: delta_pi = 0.0\n",
    "    return delta_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_theta(theta, w, x, x_dot, a, sigma):\n",
    "    '''This function updates the policy parameter, theta for the state and action \n",
    "    specified and returns delta theta'''\n",
    "    Q = compute_Q(x, x_dot, w)\n",
    "    delta_pi = delta_log_pi(a, x, x_dot, theta, sigma)\n",
    "    return delta_pi * Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_action(x, x_dot, sigma, theta): \n",
    "    '''Function computes a next action, state, reward, and done flag given a state'''\n",
    "    ## Find the action and the next state, done flag and reward\n",
    "    a = nr.normal(loc=theta[x_state(x), x_dot_state(x_dot)], scale=sigma, size=1)\n",
    "    x_prime, x_dot_prime, done, reward = sim_car(x, x_dot, a)\n",
    "    return a, x_prime, x_dot_prime, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tile_AC(episodes = 1000, gamma = 0.9, alpha = 0.02, sigma = 1.0, beta = 0.02, x_dot_knot = 0.0):\n",
    "    \n",
    "    ## Initialize the parameter arrays for w, and theta\n",
    "    ## indexed by position, velocity\n",
    "    w = np.zeros((2,20,20))\n",
    "    theta = np.zeros((20,20))\n",
    "\n",
    "    I = 1.0\n",
    "    \n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        x_dot = [x_dot_knot]\n",
    "        x = [initalize_car()]\n",
    "\n",
    "        ## Get next state, action reward, etc. for the car.\n",
    "        a, x_prime, x_dot_prime, reward, done = next_action(x[0], x_dot[0], sigma, theta)\n",
    "        x.append(x_prime)\n",
    "        x_dot.append(x_dot_prime)\n",
    "        \n",
    "       \n",
    "\n",
    "        i = 1\n",
    "        while(not done):\n",
    "            ## find the next action and state\n",
    "            a_prime, x_prime, x_dot_prime, reward_prime, done_prime = next_action(x[i], x_dot[i], sigma, theta)\n",
    "            x.append(x_prime)\n",
    "            x_dot.append(x_dot_prime)\n",
    "            \n",
    "            ## Compute the TD error and update the critic\n",
    "            delta = reward + gamma * compute_Q(x[i], x_dot[i], w) - compute_Q(x[i-1], x_dot[i-1], w)\n",
    "            d_w = np.multiply(beta * delta, np.array([x[i-1],x_dot[i-1]])).reshape((1,2)) \n",
    "            w[:, x_state(x[i-1]), x_dot_state(x_dot[i-1])] = np.add(w[:, x_state(x[i-1]), x_dot_state(x_dot[i-1])], d_w)\n",
    "            \n",
    "            ## Update the policy parameter theta of the actor\n",
    "            d_theta = delta_theta(theta, w, x[i-1], x_dot[i-1], a, sigma)\n",
    "            theta[x_state(x[i-1]), x_dot_state(x_dot[i-1])] =  theta[x_state(x[i-1]), x_dot_state(x_dot[i-1])] + alpha * d_theta\n",
    "            \n",
    "            ## Update the state variables\n",
    "            a = a_prime\n",
    "            reward = reward_prime\n",
    "            done = done_prime\n",
    "            i = i + 1\n",
    "            \n",
    "    ## Return the policy        \n",
    "    return(theta)\n",
    "            \n",
    "stochastic_policy = tile_AC()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_time_step(x_knot, x_dot_knot, policy, sigma = 1.0):\n",
    "    '''Function uses a stochastic policy to take a time step \n",
    "    as predicted by the target model. The function also returns if the episode is done''' \n",
    "    x = [x_knot]\n",
    "    x_dot = [x_dot_knot]\n",
    "    actions = [0.0]\n",
    "    done = False\n",
    "    \n",
    "    for i in range(400):\n",
    "        a, x_prime, x_dot_prime, reward, done = next_action(x[i], x_dot[i], sigma, policy)\n",
    "        i =+ 1\n",
    "        x.append(x_prime)\n",
    "        x_dot.append(x_dot_prime)\n",
    "        actions.append(a[0])\n",
    "    return x, x_dot, actions, done\n",
    "\n",
    "x_steps, x_dot_steps, action_steps, _ = policy_time_step(-0.01, 0.0, stochastic_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_car_a(x, x_dot, a):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax1 = fig.add_subplot(311)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(312)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    \n",
    "    ## PLot acceleration\n",
    "    ax2 = fig.add_subplot(313)  \n",
    "    ax2.plot(a)\n",
    "    ax2.set_ylabel('Policy parameter theta')\n",
    "    ax2.set_xlabel('Time')\n",
    "    \n",
    "plot_car_a(x_steps, x_dot_steps, action_steps)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic Methods\n",
    "\n",
    "As previously mentioned, approximate policy gradients can have high variance. We can now introduce a **baseline function** to find a policy gradient with lower variance. This approach leads to the **advantage actor-critic** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce a baseline\n",
    "\n",
    "One way to reduce variance is simply to find a formulation with a smaller range of values. As long as the expectation policy gradient is not changed, the algorithm will still converge to the correct solution.These considerations lead to the introduction of a **baseline function**.   \n",
    "\n",
    "The expectation of the policy gradient with an arbitrary baseline function can be written:  \n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\ \\big( q_\\pi(s,a) - b(s) \\big)\\\\ \n",
    "= \\mathbb{E}_{\\pi_\\theta} \\big[ \\sum_a\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\ \\big( q_\\pi(s,a) - b(s) \\big) \\big]$$\n",
    "\n",
    "Including a baseline function does not change the expectation of the gradient, as can be seen by the following relationship:  \n",
    "\n",
    "$$\\sum_a\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\ b(s) = b(s) \\sum_a\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = b(s)\\ \\nabla_\\theta 1 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The advantage function  \n",
    "\n",
    "As has been mentioned, the ideal baseline function must not change the expectation of the policy gradient. A good choice of baseline function is the **state-value**, which leads to the **advantage function**:  \n",
    "\n",
    "$$A_{\\pi_\\theta}(s,a) = Q_{\\pi_\\theta}(s,a) - V_{\\pi_\\theta}(s)$$\n",
    "\n",
    "The advantage function is the difference between the action-value function and value function. At convergence the difference between state-value and action-value is 0. Thus, variance is minimized. \n",
    "\n",
    "The lower variance policy gradient for the advantage function is:  \n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) = \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ A_{\\pi_\\theta}(s,a) \\big]$$  \n",
    "\n",
    "The parameter update is then:\n",
    "\n",
    "$$\\Delta \\theta = \\alpha\\ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ A_{\\pi_\\theta}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2019, Stephen F. Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
