{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 - Lab\n",
    "\n",
    "# Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Policy Gradients\n",
    "\n",
    "In the previous lesson we created parameterized state value and action value functions:\n",
    "\n",
    "$$V(s_t) \\approx  V(s_t,\\mathbf{w}_t) \\\\\n",
    "Q(s_t,a_t) \\approx Q(s_t,a_t,\\mathbf{w}_t)$$\n",
    "\n",
    "Where, $\\mathbf{w}$ is the parameter vector. Using these parameterized functions optimal policies are computed. \n",
    "\n",
    "Now, we will consider **parameterized policy functions** which can be written in the form:   \n",
    "\n",
    "$$\\pi(a\\ |\\ s, \\mathbf{\\theta}) = Pr\\{A_t = a\\ |\\ S_t = s, \\mathbf{\\theta_t} = \\mathbf{\\theta} \\}$$   \n",
    "\n",
    "Where, $\\mathbf{\\theta} \\in R^d$ is the d-dimensional **parameter vector**. The parameterized policy is often referred to as the **actor**.   \n",
    "\n",
    "A **parameterized value function**, $\\hat{v}(s, \\mathbf{w})$, can be used to evaluate a policy. The value function is determined by state, s, and the d-dimensional parameter vector $\\mathbf{w} \\in R^d$. The parameterized value function is often referred to as the **critic**.  \n",
    "\n",
    "\n",
    "**Actor-critic** algorithms use gradient methods to learn both the policy parameters and the value function or critic. This method leads to the name **policy gradient methods**.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterized Policy Approximation\n",
    "\n",
    "How can we parameterize a policy?   \n",
    "\n",
    "First, the parameterized policy must be **differentiable** with respect to to the parameter vector, $\\mathbf{\\theta}$, to be amenable to gradient ascent methods. To be learnable the policy gradient with respect to $\\mathbf{\\theta} \\in R^d$, $\\nabla_{\\mathbf{\\theta}} \\pi(a\\ |\\ s, \\mathbf{\\theta})$, must exist and be bounded for all states, $s \\in \\mathcal{S}$, and all actions from these states, $a \\in \\mathcal{A}(s)$.  \n",
    "\n",
    "Second, the policy must never become deterministic. In other words, the probability of taking an action must not be simply binary, $\\{ 0,1 \\}$, or $\\pi(a\\ |\\ s, \\mathbf{\\theta}) \\in \\{ 0,1 \\}$. Instead, a viable policy must allow each possible action with some probability for all states,  $0 \\gt \\pi(a\\ |\\ s, \\mathbf{\\theta}) \\gt 1.0,\\ \\forall\\ a,\\ \\forall\\ s$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of parameterized policy \n",
    "\n",
    "You may be wondering what the advantages and disadvantages of parameterized policy might be? The advantages can be summarized as:\n",
    "\n",
    "- **Improved convergence properties**. In some cases, learning a parameterized policy can be more **sample efficient** than other RL learning methods. Sample efficiency means making good use of the data samples to improve a policy.   \n",
    "- **Scalable to high dimensional and continuous action spaces**. We have investigated methods to learn policy for continuous state spaces. But, the examples we have examined to now have discrete action spaces. However, many real-world problems have continuous action spaces. Parameterized policy methods work well with continuous action spaces. \n",
    "- **Can learn a stochastic policies**. All of the algorithms we have examined until now create deterministic policies. Whereas, parameterized policy can be stochastic.   \n",
    "\n",
    "The disadvantages of a parameterized policy include:\n",
    "\n",
    "- These algorithms will **often converge to a locally optimal solution**, rather than a globally optimal solutions.\n",
    "- The estimation of policy gradient can have high variance. We will examine methods to reduce this variance. \n",
    "- Policy evaluation is relatively inefficient and has high variance. We will examine methods to reduce the variance shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy optimization   \n",
    "\n",
    "Learning with parameterized policy involves maximizing the value of the policy. A number of optimization methods have been used for this problem. Here, we will use **gradient ascent** to maximize policy value.      \n",
    "\n",
    "The goal of policy gradient methods is to learn a parameter vector, $\\mathbf{\\theta}$, which **maximizes policy value**, $J(\\mathbf{\\theta})$. The commonly used learning method is to apply **gradient ascent** method of the form:  \n",
    "\n",
    "$$\\mathbf{\\theta}_{t+_1} = \\mathbf{\\theta}_t + \\alpha \\widehat{\\nabla J(\\mathbf{\\theta})}$$  \n",
    "\n",
    "Where,\n",
    "\n",
    "$\\alpha = $ the learning rate.  \n",
    "$\\widehat{\\nabla J(\\mathbf{\\theta})} \\in R^d = $ the **estimate** of the d-dimension **gradient** vector of the loss function:\n",
    "\n",
    "$$\\widehat{\\nabla_{\\theta} J(\\mathbf{\\theta})} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_d}\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Theorem  \n",
    "\n",
    "We can find the **policy gradient** analytically, if the parameterized policy, $\\pi_{theta}$, is **differentiable and non-zero everywhere**. The gradient is then $\\nabla_{\\theta} \\pi_{\\theta}$. But, how can this gradient be found in practice? The answer is to apply the **policy gradient theorem**.   \n",
    "\n",
    "For an episodic MDP we can define the performance by the loss function:\n",
    "\n",
    "$$J(\\mathbf{\\theta}) = v_{\\pi_{\\mathbf{\\theta}}}(s_0)$$  \n",
    "\n",
    "Where $s_0$ is the starting state of the episode. In this case, there is no discounting, with $\\gamma = 1$.\n",
    "\n",
    "Given the loss function defined above the policy gradient theorem says that the gradient is:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\\\ = \\mathbb{E}_{\\pi_\\theta} \\big[ \\sum_a q_\\pi(s,a) \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio and score function\n",
    "\n",
    "There is an efficient way to find the gradient of the policy $\\nabla_\\theta \\ \\pi(a|S_t,\\mathbf{\\theta})$. Start with the **likelihood ratio**, obtained by multiplying the numerator and denominator by $\\pi(a|S_t,\\mathbf{\\theta})$:\n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\pi(a|S_t,\\mathbf{\\theta}) \\frac{ \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})}{\\pi(a|S_t,\\mathbf{\\theta}))} $$    \n",
    "\n",
    "Now, use the following identity:\n",
    "\n",
    "$$\\frac{ \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})}{\\pi(a|S_t,\\mathbf{\\theta}))} = \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})$$\n",
    "\n",
    "Where $\\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})$ is the **score function**.\n",
    "\n",
    "Substituting the score function gives: \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\pi(a|S_t,\\mathbf{\\theta}) \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Policies\n",
    "\n",
    "Algorithms we have examined previously, value iteration and policy iteration, result in **deterministic policies**. A deterministic policy takes a specific optimal action given the state. \n",
    "\n",
    "But, what happens if there is uncertainty as to the best action? In this case, a **stochastic policy** is required. As you likely intuit from the term, the action taken by a stochastic policy is probabilistic.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete stochastic actions    \n",
    "\n",
    "The deterministic policies we have examined previously all take discrete actions. A policy with deterministic discrete actions can be represented, $\\pi(a|s) \\in {0,1}$. In other words, a binary response, given the state, a specific action is either taken or not. \n",
    "\n",
    "Alternatively, the actions taken by a stochastic policy are determined probabilistically. If there are a limited number of possible actions, the probability of taking an action can be computed as **softmax action preferences**:\n",
    "\n",
    "$$\\pi(a|s, \\mathbf{\\theta}) = \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_b e^{h(s,a,\\mathbf{\\theta})}}$$   \n",
    "\n",
    "The action preferences with the largest probabilities are the most likely to be taken.    \n",
    "\n",
    "For the case of policy parameterization using linear function approximation, $\\phi(s,a)\\ \\mathbf{\\theta}$:    \n",
    "\n",
    "$$\\pi(a|s, \\mathbf{\\theta}) \\propto  e^{\\phi(s,a)^T\\ \\mathbf{\\theta}}$$\n",
    "\n",
    "Where, $\\phi(s,a)$ are the set of basis functions.\n",
    "\n",
    "The score function then becomes:  \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\phi(s,a) - \\mathbb{E}_{\\pi_\\theta} \\big[ \\phi(s,\\cdot) \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous actions and Gaussian distributions   \n",
    "\n",
    "Many real world problems have continuous action spaces. Parameterized policies are ideal for continuous action spaces. A stochastic policy for a continuous action space can be parameterized using a Gaussian distribution:    \n",
    "\n",
    "$$a \\sim \\mathcal{N} \\big( \\mu(s),\\sigma^2 \\big)$$  \n",
    "\n",
    "where, the mean action is parameterized, $u(s) = \\pi(s)^T\\ \\mathbf{\\theta}$. It is also possible to parameterize $\\sigma^2$.\n",
    "\n",
    "The score function is then:   \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\frac{\\big(a - \\mu(s) \\big) \\pi(s)}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Gradient - Reinforce  \n",
    "\n",
    "By direct application of the policy gradient theorem the **reinforce algorithm** can be developed. For each episode, the steps of the reinforce algorithm are: \n",
    "\n",
    "1. Using Monte Carlo policy evaluation, the state value, $v(s)$, is computed.  \n",
    "2. Update the policy parameters using the policy gradient theorem:\n",
    "\n",
    "$$\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t + \\alpha\\ \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\mathbf{\\theta}})\\  v_t(s)$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.  \n",
    "\n",
    "While the reinforce algorithm converges, the variance of the Monte Carlo policy gradient can be large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Variance with a Critic\n",
    "\n",
    "How can the variance of the policy gradient be reduced? One possibility is to use ]a **critic** to estimate the action-value function:\n",
    "\n",
    "$$Q_{\\pi_{\\mathbf{\\theta}}}(s,a) \\approx Q_{w}(s,a)$$\n",
    "\n",
    "The ctor-critic algorithm alternates between these steps:\n",
    "- **Critic** evaluates the current parameterized policy. The critic updates the state-value function parameters, $w$.\n",
    "- **Actor** determines the policy of actions. The policy parameters, $\\theta$, are updated using the using the critic. \n",
    "\n",
    "The interaction between the critic and the actor can lead to a significant reduction in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic with approximate policy gradient\n",
    "\n",
    "The actor-critic algorithm uses an **approximate policy gradient**.\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\approx \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a) \\big]$$  \n",
    "\n",
    "Which leads to the parameter update:   \n",
    "\n",
    "$$\\Delta \\theta = \\alpha\\ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a)$$\n",
    "\n",
    "How can the action-value, $Q_w(s,a)$, be estimated? The policy evaluation of $\\pi_\\theta$, for the parameters $\\theta$, is used. We have examine several methods for policy evaluation:   \n",
    " \n",
    "- Monte Carlo policy evaluation.\n",
    "- Temporal difference (TD) policy evaluation. \n",
    "- Least squares fitting of policy evaluation function by least squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in Actor-Critic methods\n",
    "\n",
    "Using an approximate policy gradient introduces **bias**, which can lead to poor convergence of the solution. How can one choose a value function approximation which minimizes this bias? There are two criteria which must be met:\n",
    "\n",
    "First, the value function must be **compatible** with the policy. By this we mean the following relationship should be true:\n",
    "\n",
    "$$\\nabla_w\\ Q_w(s,a) = \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)$$\n",
    "\n",
    "Second, the value function must have parameters, $\\mathbf{w}$ which minimizes the mean squared error:   \n",
    "$$\\epsilon = \\mathbb{E}_{\\pi_\\theta} \\big[ \\big( Q_{\\pi_\\theta}(s,a) - Q_w(s,a) \\big)^2 \\big]$$\n",
    "\n",
    "The above criteria leads to the following exact policy gradient that meets both:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) = \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a) \\big]$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient for Critic\n",
    "\n",
    "How is the parameter vector, **w**, for the critic updated? As with the policy parameter vector, $\\theta$, the critic parameter vector is updated using gradient ascent. \n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\ \\delta_t\\  \\nabla_w\\ Q_w(s,a)$$\n",
    "\n",
    "where,  \n",
    "$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1},a) - Q(S_t,A_t)$ is the TD error,  \n",
    "$\\nabla_w\\ Q_w(s,a)$ is the gradient of $Q_w(s,a)$ with respect to **w**.\n",
    "\n",
    "How can the gradient, $\\nabla_w\\ Q_w(s,a)$,be computed. A simple case is to represent $Q(s,a)$ using a linear function approximation: \n",
    "\n",
    "$$Q_w(s,a) = \\phi(s,a)^T \\mathbf{w}$$  \n",
    "\n",
    "Where, $\\phi(s,a)$ are the basis functions. \n",
    "\n",
    "Since the $Q_w(s,a)$ is linear in **w**:\n",
    "\n",
    "$$\\nabla_w\\ Q_w(s,a) = \\phi(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Computational Example\n",
    "\n",
    "With the foregoing theory in mind, its time to try a computational example. The goal here is to find a good policy stochastic for the mountain car problem. The stochastic policy will use a Normally distributed action, given state. \n",
    "\n",
    "The action from the stochastic policy is computed from a **Normal distribution**. The objective is to learn a policy parameterized by $\\theta$, the mean action value. The value of $\\theta$ can be positive or negative, and is bounded in the range $\\{-1.0, 1.0 \\}$. \n",
    "\n",
    "In this example, a linear parameterized policy is used. The policy is parameterized using a basis function of a non-overlapping tile grid.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain car simulator. \n",
    "\n",
    "In previous labs you have used and discussed the mountain car simulator code in the cell below. Execute this code and its test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, log\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_car(x, x_dot, acceleration, x_lims = (-1.2,0.5), x_dot_lims = (-0.07,0.07), a_lims = (-1.0,1.0)):\n",
    "    ## Check the limits on acceleration\n",
    "    if(acceleration < x_lims[0]): acceleration = -1.0\n",
    "    if(acceleration > x_lims[1]): acceleration = 1.0\n",
    "    \n",
    "    ## Compute velocity within limits\n",
    "    x_dot_prime = x_dot + 0.001 * acceleration - 0.0025 * cos(3.0 * x)\n",
    "    if(x_dot_prime < x_dot_lims[0]): x_dot_prime = x_dot_lims[0]\n",
    "    if(x_dot_prime > x_dot_lims[1]): x_dot_prime = x_dot_lims[1]\n",
    "        \n",
    "    ## Now update position\n",
    "    x_prime = x + x_dot\n",
    "    if(x_prime < x_lims[0]): x_prime = x_lims[0]\n",
    "    if(x_prime > x_lims[1]): x_prime = x_lims[1]\n",
    "      \n",
    "    ## At the terminal state or not and set reward\n",
    "    if(x_prime >= x_lims[1]): \n",
    "        done = True\n",
    "        reward = 100.0\n",
    "    else: \n",
    "        done = False\n",
    "        reward = -1.0\n",
    "        \n",
    "    return(x_prime, x_dot_prime, done, reward)    \n",
    "        \n",
    "def initalize_car(x_lims = (-0.6,-0.4)):\n",
    "    ## Find random start for car\n",
    "    return(nr.uniform(x_lims[0],x_lims[1]))\n",
    "\n",
    "## Test the function\n",
    "a = -0.0\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "for i in range(100):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "def plot_car(x, x_dot):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(212)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "    \n",
    "plot_car(x,x_dot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title basis functions\n",
    "\n",
    "The policy to be parameterized for this example uses none-overlapping tiles as a basis function. For the two state variables, position and velocity. The tile scheme is a 20x20 grid.   \n",
    "\n",
    "The code in the cell below contains the coding function for the tile coding along one dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_state(x, x_lims = (-1.2,0.5), n_tiles = 20):\n",
    "    \"\"\"Function to compute tile state given positon\"\"\"\n",
    "    state = int((x - x_lims[0])/(x_lims[1] - x_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-1.2,0.5,20)):\n",
    "    print('x = ' + str(x) + ' state = ' + str(x_state(x)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient learning\n",
    "\n",
    "With the coding for the policy parameters available, it is time to create the code to use policy gradient methods to update these parameters. \n",
    "\n",
    "The first step, is to define a function to compute the weight updates of the critic. The critic evaluates the state-values of the policy. The function returns the TD update of the weight. Examine the code in the cell below, and execute it to load this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q(x, x_dot, w):\n",
    "    '''Function to compute action value, Q, given state and parameter vector w'''\n",
    "    return w[0, x_state(x), x_state(x_dot, x_lims = (-0.07,0.07))] * x + w[1, x_state(x), x_state(x_dot, x_lims = (-0.07,0.07))] * x_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy parameter or mean action, $\\theta$, is updated following the policy gradient theorem. The function in the cell below computes the value of $\\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)$. \n",
    "\n",
    "Since the policy parameter, $\\theta$, can be positive or negative an offset is added (given the sign) the update. The offset allows the log to be computed for negative values of the policy gradient. \n",
    "\n",
    "Examine this code and execute it, to load the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_log_pi(a, x, x_dot, theta, sigma, val_lims = (-1.0,1.0)):\n",
    "    '''This function computes the gradients of the log probability\n",
    "    for the policy given the state and action. The function assumes\n",
    "    a stochastic Gaussina distributed action space'''\n",
    "    delta_pi = (a - theta[x_state(x), x_state(x_dot, x_lims = (-0.07,0.07))])/sigma\n",
    "    delta_pi = delta_pi - val_lims[0]  # Add and offset\n",
    "    ## Take the log\n",
    "    if(delta_pi > 0.0): delta_pi = log(delta_pi)\n",
    "    else: delta_pi = 0.0\n",
    "    return delta_pi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: compute the policy parameter update\n",
    "\n",
    "With a function available to compute $\\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)$, the parameter update can now be computed. The function in the cell below uses the `delta_log_pi` function to update the policy parameter by applying the policy gradient theorem. \n",
    "\n",
    "The code to compute the parameter update is missing from the function. Add the missing code and execute it, to load your completed function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_theta(theta, w, x, x_dot, a, sigma):\n",
    "    '''This function updates the policy parameter, theta for the state and action \n",
    "    specified and returns delta theta'''\n",
    "    Q = compute_Q(x, x_dot, w)\n",
    "    delta_pi = delta_log_pi(a, x, x_dot, theta, sigma)\n",
    "    ##### ADD THE MISSING CODE BELOW SO RETURN THE UPDATE THE POLICY PARAMETER. ###########\n",
    "    return delta_pi * Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below defines a function to compute the next stochastic action, given the state and the policy parameters. There are two steps:\n",
    "\n",
    "1. The action is drawn from a Normal distribution. \n",
    "2. The state is updated, the reward found and the done flag set, based on the stochastic action. \n",
    "\n",
    "Examine this code and execute it to load the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_action(x, x_dot, sigma, theta): \n",
    "    '''Function computes a next action, state, reward, and done flag given a state'''\n",
    "    ## Find the action and the next state, done flag and reward\n",
    "    a = nr.normal(loc=theta[x_state(x), x_state(x_dot, x_lims = (-0.07,0.07))], scale=sigma, size=1)\n",
    "    x_prime, x_dot_prime, done, reward = sim_car(x, x_dot, a)\n",
    "    return a, x_prime, x_dot_prime, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Parameter updating\n",
    "\n",
    "Finally, everything is in place for the main actor-critic learning function. The steps executed are as follows:\n",
    "\n",
    "1. The weight and the policy parameter arrays are initialized. \n",
    "2. An outer loop is executed for the number of episodes specified. \n",
    "3. The initial conditions for each episode are set.\n",
    "4. The inner loop executes until the car arrives at the terminal state with the following steps: \n",
    "  - The next state, action, reward and done flag are found.\n",
    "  - The TD error is computed and the critic weights are updated.\n",
    "  - The actor (policy) parameter is updated using the critic.\n",
    "  - Limits are imposed on the policy parameter.\n",
    "  - State variables are updated for the next time step. \n",
    "\n",
    "The code for the policy parameter update is missing. Complete and execute this code and examine the results. Expect the execution to take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tile_AC(episodes = 2000, gamma = 0.9, alpha = 0.02, sigma = 1.0, beta = 0.02, \n",
    "            x_dot_knot = 0.0, theta_lims = (-1.0,1.0)):\n",
    "    \n",
    "    ## Initialize the parameter arrays for w, and theta\n",
    "    ## indexed by position, velocity\n",
    "    w = np.zeros((2,20,20))\n",
    "    theta = np.zeros((20,20))\n",
    "\n",
    "    I = 1.0\n",
    "    \n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        x_dot = [x_dot_knot]\n",
    "        x = [initalize_car()]\n",
    "\n",
    "        ## Get next state, action reward, etc. for the car.\n",
    "        a, x_prime, x_dot_prime, reward, done = next_action(x[0], x_dot[0], sigma, theta)\n",
    "        x.append(x_prime)\n",
    "        x_dot.append(x_dot_prime)\n",
    "        \n",
    "        i = 1\n",
    "        while(not done):\n",
    "            ## find the next action and state\n",
    "            a_prime, x_prime, x_dot_prime, reward_prime, done_prime = next_action(x[i], x_dot[i], sigma, theta)\n",
    "            x.append(x_prime)\n",
    "            x_dot.append(x_dot_prime)\n",
    "            \n",
    "            ## Compute the TD error and update the critic\n",
    "            delta = reward + gamma * compute_Q(x[i], x_dot[i], w) - compute_Q(x[i-1], x_dot[i-1], w)\n",
    "            d_w = np.multiply(beta * delta, np.array([x[i-1],x_dot[i-1]])).reshape((1,2)) \n",
    "            w[:, x_state(x[i-1]), x_state(x_dot[i-1], x_lims = (-0.07,0.07))] = np.add(w[:, x_state(x[i-1]), x_state(x_dot[i-1], x_lims = (-0.07,0.07))], d_w)\n",
    "            \n",
    "            ## Update the policy parameter theta of the actor\n",
    "            d_theta = delta_theta(theta, w, x[i-1], x_dot[i-1], a, sigma)\n",
    "            ###### THE CODE TO COMPUTE THE UPDATED POLICY PARAMETER VALUE theta_temp ##########\n",
    "            ###### USING THE VALUE OF d_theta MUST BE ADDED BELOW ##########\n",
    "            theta_temp =  theta[x_state(x[i-1]), x_state(x_dot[i-1], x_lims = (-0.07,0.07))] + alpha * d_theta\n",
    "            \n",
    "            ## Enforce the limits on the paramter\n",
    "            if(theta_temp > theta_lims[1]): theta[x_state(x[i-1]), x_state(x_dot[i-1], x_lims = (-0.07,0.07))] = theta_lims[1]\n",
    "            elif(theta_temp < theta_lims[0]): theta[x_state(x[i-1]), x_state(x_dot[i-1], x_lims = (-0.07,0.07))] = theta_lims[0]\n",
    "            else: theta[x_state(x[i-1]), x_state(x_dot[i-1], x_lims = (-0.07,0.07))] = theta_temp\n",
    "            \n",
    "            ## Update the state variables\n",
    "            a = a_prime\n",
    "            reward = reward_prime\n",
    "            done = done_prime\n",
    "            i = i + 1\n",
    "            \n",
    "    ## Return the policy        \n",
    "    return(theta)\n",
    "            \n",
    "stochastic_policy = tile_AC()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the stochastic policy\n",
    "\n",
    "The final task is to test the policy you just created. \n",
    "\n",
    "The function in the cell below takes the specified number of time steps for the car following the stochastic policy. The actions are found using the `next_action` function. The time series of state variables actions are returned in lists.  \n",
    "\n",
    "Execute this code to compute the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_time_step(x_knot, x_dot_knot, policy, steps = 150, sigma = 0.25):\n",
    "    '''Function uses a stochastic policy to take a time step \n",
    "    as predicted by the target model. The function also returns if the episode is done''' \n",
    "    x = [x_knot]\n",
    "    x_dot = [x_dot_knot]\n",
    "    actions = [0.0]\n",
    "    done = False\n",
    "    \n",
    "    for i in range(steps):\n",
    "        a, x_prime, x_dot_prime, reward, done = next_action(x[i], x_dot[i], sigma, policy)\n",
    "        i =+ 1\n",
    "        x.append(x_prime)\n",
    "        x_dot.append(x_dot_prime)\n",
    "        actions.append(a[0])\n",
    "    return x, x_dot, actions, done\n",
    "\n",
    "nr.seed(5467)\n",
    "x_steps, x_dot_steps, action_steps, _ = policy_time_step(-0.01, 0.0, stochastic_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below creates time series plots of the state variables and the actions. In addition, a smoothed plot of the actions is supper imposed on the time series plot of the actions. Execute the code and examine the resulting plots.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def plot_car_a(x, x_dot, a):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax1 = fig.add_subplot(311)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(312)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    \n",
    "    ## PLot acceleration from policy\n",
    "    ax3 = fig.add_subplot(313)  \n",
    "    ax3.plot(a)\n",
    "    ax3.set_ylabel('Policy parameter theta')\n",
    "    ax3.set_xlabel('Time')\n",
    "    \n",
    "    ## Plot a smoothed policy curve\n",
    "    x = np.linspace(0, 50, 251)\n",
    "    smoothed = savgol_filter(a, 25, 3)\n",
    "    ax3.plot(smoothed)\n",
    "    \n",
    "plot_car_a(x_steps, x_dot_steps, action_steps)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to display the tile grid of policy parameters. Execute the code in the cell below display this grid and examine the results. Keep in mind that the axis labels correspond to the indicies of the grid, not physical values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy):\n",
    "    plt.imshow(policy)\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.title('Average acceleration given position and velocity')\n",
    "    plt.colorbar()\n",
    "    \n",
    "display_policy(stochastic_policy)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic Methods\n",
    "\n",
    "As previously mentioned, approximate policy gradients can have high variance. We can now introduce a **baseline function** to find a policy gradient with lower variance. This approach leads to the **advantage actor-critic** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce a baseline\n",
    "\n",
    "One way to reduce variance is simply to find a formulation with a smaller range of values. As long as the expectation policy gradient is not changed, the algorithm will still converge to the correct solution.These considerations lead to the introduction of a **baseline function**.   \n",
    "\n",
    "The expectation of the policy gradient with an arbitrary baseline function can be written:  \n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\ \\big( q_\\pi(s,a) - b(s) \\big)\\\\ \n",
    "= \\mathbb{E}_{\\pi_\\theta} \\big[ \\sum_a\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\ \\big( q_\\pi(s,a) - b(s) \\big) \\big]$$\n",
    "\n",
    "Including a baseline function does not change the expectation of the gradient, as can be seen by the following relationship:  \n",
    "\n",
    "$$\\sum_a\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\ b(s) = b(s) \\sum_a\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = b(s)\\ \\nabla_\\theta 1 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The advantage function  \n",
    "\n",
    "As has been mentioned, the ideal baseline function must not change the expectation of the policy gradient. A good choice of baseline function is the **state-value**, which leads to the **advantage function**:  \n",
    "\n",
    "$$A_{\\pi_\\theta}(s,a) = Q_{\\pi_\\theta}(s,a) - V_{\\pi_\\theta}(s)$$\n",
    "\n",
    "The advantage function is the difference between the action-value function and value function. At convergence the difference between state-value and action-value is 0. Thus, variance is minimized. \n",
    "\n",
    "The lower variance policy gradient for the advantage function is:  \n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) = \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ A_{\\pi_\\theta}(s,a) \\big]$$  \n",
    "\n",
    "The parameter update is then:\n",
    "\n",
    "$$\\Delta \\theta = \\alpha\\ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ A_{\\pi_\\theta}(s,a)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
