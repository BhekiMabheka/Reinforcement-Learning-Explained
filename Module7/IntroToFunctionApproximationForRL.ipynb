{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 - Lab, Part 1\n",
    "\n",
    "# Introduction to Reinforcement Learning with Function Approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now in this course we have only considered reinforcement learning problems with a small number of **discrete states**. These problems only required **tabular algorithms** for the solution of RL problems. However, tabular methods will not work in cases where the tables are too large to be held in computer memory, the computation time to sweep across states is too great, or where states do not have discrete values. \n",
    "\n",
    "But, many practical problems have either very large numbers of discrete states or have continuous states. Some example of such problems:  \n",
    "1. A Game such as chess or backgammon with a great number of possible board states. While the number of states is **countably finite** there are far too many for tabular solutions. For example, consider that each piece in a chess game can occupy any of 64 positions, and that there are anywhere between 32 pieces and 2 pieces on the board at a given time step. This situation leads to an explosion in possible states. Other games, such a Go, have far more states than chess.   \n",
    "2. A simple flight control system for a drone where the continuous state variables have an infinite number of states. These state variables would include 3-dimensional velocity, acceleration, and position, along with pitch yaw and role. All 12 of these variables have **continuous values** and, thus, an **infinite number of states**.   \n",
    "\n",
    "To address such problems we must have a better representations. In this lesson we will focus on a powerful class of representations known as **function approximation**. By using function approximation we can represent a large number of states, even an infinite number, with a limited number of **parameters**. Function approximation RL problems of great complexity can be tackled, at least in theory.  \n",
    "\n",
    "A reinforcement learning agent using function approximation is illustrated schematically in the figure below. \n",
    "\n",
    "<img src=\"img/AgentEnvironment.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent with Function Approximation Representation and Environment** </center>  \n",
    "\n",
    "In this lesson we will only address function approximation methods for **episodic** tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation for Non-Tabular RL\n",
    "\n",
    "The main question we will address in this lesson is what representations can we use when tabular methods will not work. The idea key idea is to **encode** a large, possibly infinite, number of states using a **few parameters**. Thus, **state values** or **action values** are represented by some **function of the state** of the environment.  \n",
    "\n",
    "There are, in fact, many possible approaches to this problem. Here we will focus on function approximation methods of which a few examples are: \n",
    "1. Simple **linear** and **polynomial** representations,\n",
    "2. **Fourier basis function** representations,\n",
    "3. **Course coding** using overlapping circular or elliptical regions,\n",
    "4. Various forms of **tile coding**,\n",
    "5. **Radial basis functions** or kernels,\n",
    "5. **Fully connected** deep neural networks, \n",
    "6. **Convolutional** deep neural networks. \n",
    "\n",
    "Each of the first five methods involve **coding** using some type of **basis function**. Basis functions rely on an implicit assumption that **nearby states have similar values**.  The representation is then **linear in the parameters** and **linear solution methods** are used to find these parameters. Linear methods have the advantage of computational efficiency. Further, at least for **on-policy** algorithms, **convergence is guaranteed**.     \n",
    "\n",
    "The deep neural networks provide a distinctly nonlinear function approximation method. Typically, deep neural networks are used for **off-policy Q-Learning** methods. The convergence properties of these algorithms can be problematic. In fact, there are few guarantees of convergence with **off-policy function approximation** algorithms. The use of deep neural networks for Q-learning are the topic of Part 2 of this lab.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile coding\n",
    "\n",
    "Tile coding is a flexible and expressive function approximation method. The basic idea is simple. The **state space is divided** into small patches using a regular pattern of **geometric shapes** or **tiles**. The function approximation has one parameter (weight) for each tile.    \n",
    "\n",
    "An example tile coding of a two dimensional state space is shown in the figure below. In this case, a uniform 8x8 set of tiles are used, leading to a representation with 64 parameters. \n",
    "\n",
    "<img src=\"img/Tile1.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Two-dimensional state space encoded by 8x8 rectangular tiles** </center>\n",
    "\n",
    "In the above diagram the states shown by **X** are in the same tile and will be coded with the same parameter. The states show by **O** are in different tiles and are coded with different parameters. \n",
    "\n",
    "However, the tile codings are far from unique. Consider the tiling of the same state space shown in the figure below. In this case the tiles are a 4X16 grid. As in the first case, there are still 64 parameters. \n",
    "\n",
    "<img src=\"img/Tile2.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Two-dimensional state space encoded by 4x16 rectangular tiles** </center>\n",
    "\n",
    "In the above coding the states shown with **O** are in the same tile and represented by the same parameter. But, the states shown as **X** are now in different tiles and are represented by different parameters. \n",
    "\n",
    "A great many tile coding schemes are possible. Commonly, multiple tiling schemes are used simultaneously. This practice allows for the capture of information at **different scales**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximation Error\n",
    "\n",
    "As already noted, when tabular algorithms are not feasible, you must resort to function approximation. You can use function approximation for both state values, $\\hat{v}(s)$, and action values, $\\hat{q}(s,a)$. You should always keep in mind that we are dealing with **approximations** and will never know the true state values, $v_{\\pi}(s)$, and action values, $q_{\\pi}(s,a)$. There will always be some **error** between the true values and the approximated values. For example, for state value approximation we can express this error as the **mean squared value error**:\n",
    "\n",
    "$$\\overline{VE}(w) = \\sum_{s \\in S} \\mu(s) \\Big[ v_{\\pi}(s) -  \\hat{v}(s,\\mathbf{w}) \\Big]^2$$  \n",
    "\n",
    "Here, $\\mu(s)$ is a weight indicating how important the state $s$ is. For on-policy RL, $\\mu(s)$ is a probability known as the **on-policy distribution**. \n",
    "\n",
    "A similar error metric can be constructed for $\\hat{q}(s,a)$.  \n",
    "\n",
    "In practical terms there is a trade-off between the complexity of the approximate representation and the error. More complex representations require more parameters, but have lower error and vice versa. This situation is exactly analogous to the bias-variance trade-off, familiar from machine learning.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function and Stochastic Gradient Decent\n",
    "\n",
    "Following the foregoing discussion, basis function methods are linear in their parameters. In the following we will express these parameters as a vector of model weights, $\\mathbf{w}$. We can then represent the approximate state value function with state $x(s)$ as:\n",
    "\n",
    "$$\\hat{v}(s,\\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s) =  \\sum_{i=1}^d   w_i  x_i(s)$$\n",
    "\n",
    "In principle, **stochastic gradient decent** algorithms are an efficient way to solve for the weights, $\\mathbf{w}$. At each step the update is just:\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\ E_{\\hat{p}data}\\Big[ \\nabla_{w} J(\\mathbf{w}_t) \\Big]\\\\\n",
    "= \\mathbf{w}_t + \\alpha  \\big[v_{\\pi}(s) - \\hat{v}(S_t, \\mathbf{w}_t) \\big]\\nabla_w \\hat{v}(S_t,\\mathbf{w}_t)$$\n",
    "\n",
    "where, $\\hat{p}data$ is the mini-batch, and gradient is given by:   \n",
    "\n",
    "$$\\nabla_w \\hat{v}(S_t,\\mathbf{w}_t) = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_1} \\\\\n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\hat{v}(S_t,\\mathbf{w}_t)}{\\partial w_d}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "In many practical algorithms, $\\hat{v}(s)$ is a **bootstrapped** approximation. This means the error term and gradient are not exact. We call such algorithms **semi-gradient decent** methods as they use an approximation of the gradient. This approach generally works well, but does not have the strong convergence guarantees of stochastic gradient decent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mountain Car Problem\n",
    "\n",
    "The [mountain car problem](https://en.wikipedia.org/wiki/Mountain_car_problem) was first proposed in the [Andew Moore's Ph.D. dissertation (1990)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.2654). The mountain car problem has become a canonical testbed for many reinforcement learning algorithms. \n",
    "\n",
    "In this problem an under-powered car must climb a steep hill. However, the car does not have sufficient engine power to climb the grade. The car must travel up another hill in order to gain sufficient speed (actually kinetic energy) to climb the large hill. \n",
    "\n",
    "The position, $x$, and velocity, $\\dot{x}$, of the car are the state variables. The updates of the state variables at each time step are determined by the following equations: \n",
    "\n",
    "$$x' = x + \\dot{x} \\\\\n",
    "\\dot{x}' = \\dot{x} + 0.001 * \\ddot{x} - 0.0025 * cos(3 * x)$$\n",
    "\n",
    "The object of this problem is to find the optimal acceleration given the car state to allow the car to get to the top of the hill. The car has three acceleration states, $\\ddot{x}$, which must be selected by the agent:\n",
    "\n",
    "$$\\ddot{x} = \\{ -1.0, 0.0, 1.0 \\}$$\n",
    "\n",
    "The position and velocity are bounded, with the goal at the upper bound of position:\n",
    "\n",
    "$$-1.2 \\le x \\le 0.5 \\\\\n",
    "-0.07 \\le \\dot{x} \\le 0.07$$\n",
    "\n",
    "The reward at each time step is -1.0 and the reward for reaching the goal is 100.  \n",
    "\n",
    "The car is randomly initialized using a uniform distribution:\n",
    "\n",
    "$$p(x_0) = uniform(-0.6 \\le x_0 \\le -0.4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mountain car problem is hard\n",
    "\n",
    "At first glance, the mountain car problem may seem like it should have an easy solution. However, looks are deceptive. The learning an optimal policy for this problem is difficult. In fact, conventional control theory approaches fail to provide solutions. Some reasons for this difficulty include:\n",
    "1. The non-linear coupling between the two state variables, which makes the state transitions between the infinite number of states hard to predict.\n",
    "2. The delayed reward which is only observed when the goal is achieved. This fact is common to many difficult RL problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation of car environment\n",
    "\n",
    "The code in the cell below **simulates the car environment**. Two functions are used by the agent to interact with the environment:\n",
    "1. The `sim_car` function returns a state transition and a reward, given the agent's current state and an action. In addition, a flag is returned to indicate if the goal has been reached.\n",
    "2. The `initialize_car` function returns a random starting position for the car within the specified bounds. \n",
    "\n",
    "Taken together, calls to these two functions define the **boundary between the agent and the environment**. Execute the code in the cell below to exercise these functions and examine the resulting plots for a case where the acceleration is set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import cos\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_car(x, x_dot, acceleration, x_lims = (-1.2,0.5), x_dot_lims = (-0.07,0.07)):\n",
    "    ## Compute velocity within limits\n",
    "    x_dot_prime = x_dot + 0.001 * acceleration - 0.0025 * cos(3 * x)\n",
    "    if(x_dot_prime < x_dot_lims[0]): x_dot_prime = x_dot_lims[0]\n",
    "    if(x_dot_prime > x_dot_lims[1]): x_dot_prime = x_dot_lims[1]\n",
    "        \n",
    "    ## Now update position\n",
    "    x_prime = x + x_dot\n",
    "    if(x_prime < x_lims[0]): x_prime = x_lims[0]\n",
    "    if(x_prime > x_lims[1]): x_prime = x_lims[1]\n",
    "      \n",
    "    ## At the terminal state or not and set reward\n",
    "    if(x_prime >= x_lims[1]): \n",
    "        done = True\n",
    "        reward = 100.0\n",
    "    else: \n",
    "        done = False\n",
    "        reward = -1.0\n",
    "        \n",
    "    return(x_prime, x_dot_prime, done, reward)    \n",
    "        \n",
    "def initalize_car(x_lims = (-0.6,-0.4)):\n",
    "    ## Find random start for car\n",
    "    return(nr.uniform(x_lims[0],x_lims[1]))\n",
    "\n",
    "## Test the function\n",
    "a = -0.0\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "for i in range(100):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "def plot_car(x, x_dot):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(212)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "    \n",
    "plot_car(x,x_dot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no acceleration applied, the car oscillates back and forth. The motion is not damped since the simulator includes no friction term. \n",
    "\n",
    "Next, execute the code in the cell below and observe the effect of using a constant positive acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the function with positive acceleration\n",
    "a = 1.0\n",
    "x_dot = [0.0]\n",
    "#x = [initalize_car()]\n",
    "x = [0.3]\n",
    "for i in range(200):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "plot_car(x,x_dot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the motion of the car is periodic initially. Finally, the car reaches the top of the hill. \n",
    "\n",
    "Finally, execute the code in the cell below to observe the effect of negative acceleration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the function with negative acceleration\n",
    "a = -1.0\n",
    "x_dot = [0.0]\n",
    "#x = [initalize_car()]\n",
    "x = [0.3]\n",
    "for i in range(500):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "plot_car(x,x_dot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position state of the car, quickly hits the negative limit. It then oscillates periodically thereafter approaching, but not quite hitting the limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic mountain car solution\n",
    "\n",
    "While learning optimal policy for the mountain car problem is rather difficult, it is not to hard to find a good heuristic solution. There are two basic observations on which to base a heuristic:\n",
    "1. The car should accelerate in the direction of the slope when traveling up hill to maximize the height to which it can climb. \n",
    "2. The car should accelerate in the opposite direction to the slope when traveling down hill to maximize the speed the car has for climbing the opposite hill. \n",
    "\n",
    "The code in the cell below implements this heuristic and plots the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_car_a(x, x_dot, a):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax1 = fig.add_subplot(311)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(312)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    \n",
    "    ## PLot acceleration\n",
    "    ax2 = fig.add_subplot(313)  \n",
    "    ax2.plot(a)\n",
    "    ax2.set_ylabel('Acceleration of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "\n",
    "    \n",
    "def heuristic_control(x, x_dot):\n",
    "    ## Set acceleration to +1, -1 or 0 based on postion and velocity\n",
    "    if((x > 0.0 and x_dot > 0.0) or (x < 0.0 and x_dot > 0.0)): a = +1.0\n",
    "    elif((x < 0.0 and x_dot < 0.0) or (x > 0.0 and x_dot < 0.0)): a = -1.0\n",
    "    else: a = 0.0\n",
    "    return(a)    \n",
    "\n",
    "        \n",
    "## Initialize the car state\n",
    "a = [0.0]\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "print('Starting positon = ' + str(x[0]))\n",
    "\n",
    "## Iterate until termination\n",
    "i = 0    \n",
    "done = False\n",
    "while not done:\n",
    "    ## Update the position and velocity    \n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a[i])\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "        \n",
    "    ## Update acceleration using the heuristic\n",
    "    i = i + 1    \n",
    "    a.append(heuristic_control(x[i], x_dot[i]))\n",
    "        \n",
    "plot_car_a(x,x_dot,a)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plots produced. The car position chart shows a fairly quick convergence to the goal state. The acceleration chart shows the heuristic policy is followed. However, this cannot be considered an optimal policy. Notice that the car stays at nearly constant position of about -1.2 for a number of time steps. Nothing was gained in terms of achieving the goal in this period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Coding N-Step SARSA Solution for Mountain Car Problem\n",
    "\n",
    "Now, we will apply tile coding along with N-step SARSA to the solution of the mountain car problem. Using tile coding results in a representation linear in the weights for each tile. \n",
    "\n",
    "### Tile coding the state variables\n",
    "\n",
    "There are two state variables for the mountain car problem, position and velocity. In this case, we will use a 10x10 grid of uniform tiles for each of the 3 actions. This coding results in **300 weights** representing the **action values** for each possible combination of the state variables. \n",
    "\n",
    "The code in the cell below computes the tile assignment for the velocity state variable, $x$. The range of possible values within the velocity limits is divided into 10 equal segments. Execute the code and examine the results of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_state(x, x_lims = (-1.2,0.5), n_tiles = 10):\n",
    "    \"\"\"Function to compute tile state given positon\"\"\"\n",
    "    state = int((x - x_lims[0])/(x_lims[1] - x_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-1.2,0.5,20)):\n",
    "    print('x = ' + str(x) + ' state = ' + str(x_state(x)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below tile encodes the velocity state variable into 10 equal segments between the limits. Execute this code and examine the results of the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_dot_state(x_dot, x_dot_lims = (-0.07,0.07), n_tiles = 10):\n",
    "    \"\"\"Function to compute tile state given velocity\"\"\"\n",
    "    state = int((x_dot - x_dot_lims[0])/(x_dot_lims[1] - x_dot_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-0.07,0.07,20)):\n",
    "    print('x_dot = ' + str(x) + ' state = ' + str(x_dot_state(x)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions provide a lookup method for the index of the tile along the two state variable dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Step SARSA for Tile Coding\n",
    "\n",
    "The code in the cell below implements N-step SARSA for the simple tile coding scheme. The N-step SARSA learns the weights for the tiles. Technically, this is a **semi-gradient decent** algorithm since bootstrapped values are used for the weight updates. \n",
    "\n",
    "The **approximate action value** for a given state and action, $x_i(s,a)$, given the $d$ tile weights, $w_i$, can be computed as:\n",
    "\n",
    "$$q(s,a) \\approx \\hat{q}(s,a,\\mathbf{w}) = \\sum_{i=1}^d  w_i  x_i(s,a)$$\n",
    "\n",
    "The state action variables, $x_i(s,a)$, are binary indicators:\n",
    "\n",
    "$$x_i(s,a) = 1\\ if\\ in\\ tile\\ i\\\\\n",
    "x_i(s,a) = 0\\ otherwise$$\n",
    "\n",
    "The weight update for the N-step SARSA tile coding algorithm, given rewards $R_t$, the N-step bootstrapped return is expressed as:  \n",
    "\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1})$$\n",
    "\n",
    "Using this return, the action value update becomes:\n",
    "\n",
    "$$w_{t+n}(S_t, A_t) = w_{t+n-1} + \\alpha \\big[ G_{t:t+n} -\\hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1}) \\big]\\ \\nabla\\hat{q}(S_{t}, A_{t}, w_{t+n-1})$$\n",
    "\n",
    "Where   \n",
    "$\\delta_t =  G_{t:t+n} - \\hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1}) = $ the n-step TD error, and   \n",
    "$\\nabla\\hat{q}(S_{t}, A_{t}, w_{t+n-1}) = $ the gradient of the action value approximation. \n",
    "\n",
    "Given the linear relationship between $\\hat{q}(S_{t}, A_{t}, w_{t+n-1})$ and $w_{t+n-1}$ the gradient is easy to compute: $\\nabla\\hat{q}(S_{t}, A_{t}, w_{t+n-1}) = 1$ for cases where $x_i(s,a) = 1$.\n",
    "\n",
    "In the algorithm presented, actions are selected using an $\\epsilon$-greedy approach. \n",
    "\n",
    "As with other N-step SARSA algorithms there as a bit of bookkeeping. Further details of this algorithm can been obtained by reading the code comments. \n",
    "\n",
    "Execute this code and examine the results of the test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tile_SARSA(episodes = 1000, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "               n = 4, goal = 0.5, a_knot = 0.0, x_dot_knot = 0.0):\n",
    "    ## Possible actions\n",
    "    actions = [-1.0,0.0,1.0]\n",
    "    \n",
    "    ## Initialize the weight array for action, position, velocity\n",
    "    w = np.zeros((3,10,10))\n",
    "\n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        a = [a_knot]\n",
    "        a_index = 1\n",
    "        x_dot = [x_dot_knot]\n",
    "        x_dot_index = x_dot_state(x_dot[0])\n",
    "        x = [initalize_car()]\n",
    "        x_index = x_state(x[0])\n",
    "        \n",
    "        t = 0 # Initialize the time step count\n",
    "        T = float(\"inf\")\n",
    "        tau = 0\n",
    "        reward_list = [] \n",
    "        \n",
    "        done = False\n",
    "        i = 0 # Index for accumulting position and velocity steps\n",
    "        while(not done):\n",
    "            if(t < T):\n",
    "                ## Update postion and velocity of the car given action and get the reward\n",
    "                x_prime, x_dot_prime, done, reward = sim_car(x[i], x_dot[i], a[i])\n",
    "                reward_list.append(reward)  # append the reward to the list\n",
    "        \n",
    "                ## The next state given the action\n",
    "                x_prime_index = x_state(x_prime)\n",
    "                x_dot_prime_index = x_dot_state(x_dot_prime)\n",
    "        \n",
    "            if(done): T = t + 1  # We reached the terminal state\n",
    "            else:\n",
    "                # Select and store the next action using the policy with epslion greedy approach\n",
    "                if(nr.uniform() > epsilon):\n",
    "                    a_prime_index = np.where(w[:,x_prime_index,x_dot_prime_index] == np.max(w[:,x_prime_index,x_dot_prime_index]))\n",
    "                    ## break the tie if needed\n",
    "                    if(a_prime_index[0].shape[0] > 1): a_prime_index = nr.choice(a_prime_index[0])\n",
    "                    else: a_prime_index = a_prime_index[0][0] \n",
    "                    a_prime = actions[a_prime_index]\n",
    "                else:\n",
    "                    a_prime_index = nr.choice(range(3)) #[0]\n",
    "                    a_prime = actions[a_prime_index]\n",
    "           \n",
    "            tau = t - n + 1 ## update the time step being updated\n",
    "  \n",
    "            if(tau >= 0): # Check if enough time steps to compute return\n",
    "                ## Compute the return\n",
    "                ## The formula for the first index in the loop is different from Sutton and Barto\n",
    "                ## but seems to be correct at least for Python.\n",
    "                G = 0.0 \n",
    "                for j in range(tau, min(tau + n, T)):\n",
    "                    G = G + gamma**(j-tau) * reward_list[j]   \n",
    "                ## Deal with case of where we are not at the terminal state\n",
    "                if(tau + n < T): G = G + gamma**n * w[a_prime_index,x_prime_index,x_dot_prime_index] \n",
    "                ## Finally, update w\n",
    "                w[a_index,x_index,x_dot_index] = w[a_index,x_index,x_dot_index] + alpha * (G - w[a_index,x_index,x_dot_index])        \n",
    "    \n",
    "            ## Set action and state for next iteration\n",
    "            if(x_prime <= goal):\n",
    "                a_index = a_prime_index\n",
    "                x_index = x_prime_index\n",
    "                x_dot_index = x_dot_prime_index  \n",
    "                a.append(a_prime) \n",
    "                x.append(x_prime)\n",
    "                x_dot.append(x_dot_prime)\n",
    "                i = i + 1\n",
    "\n",
    "            ## increment t\n",
    "            t = t + 1\n",
    "    return(w)  \n",
    "\n",
    "print(np.round(tile_SARSA(episodes = 10000), 2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 300 weight values represented in the above array. Notice that generally the weights, and thus the action values, increase with larger values of position which are closer to the goal. \n",
    "\n",
    "The code in the cell below uses the results of N-step SARSA to compute a policy. The action is selected in a greedy manner based on the action values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tile_SARSA_policy(episodes = 1000, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "                   n = 4, goal = 0.5, a_knot = 0.0, x_dot_knot = 0.0):\n",
    "    ## Possible actions\n",
    "    actions = [-1.0,0.0,1.0]\n",
    "    \n",
    "    ## Find action-values with SARSA\n",
    "    Q = tile_SARSA(episodes = episodes, gamma = gamma, epsilon = epsilon, alpha = alpha, \n",
    "                         n = n, goal = goal, a_knot = a_knot, x_dot_knot = x_dot_knot)\n",
    "    Q_shape = Q.shape\n",
    "    \n",
    "    ## Array to hold policy\n",
    "    policy = np.zeros((Q_shape[1],Q_shape[2]))\n",
    "    \n",
    "    ## Loop over postions\n",
    "    for x_index in range(Q_shape[1]):\n",
    "        ## Loop over velocity\n",
    "        for x_dot_index in range(Q_shape[2]):\n",
    "            ## Find the index with max Q\n",
    "            a_index = np.where(Q[:,x_index,x_dot_index] == np.max(Q[:,x_index,x_dot_index]))\n",
    "            ## Break the tie by using 0 acceleration\n",
    "            if(a_index[0].shape[0] > 1 ): a_index = 1\n",
    "            else: a_index = a_index[0][0]    \n",
    "            \n",
    "            ## Now fill in the policy array\n",
    "            policy[x_index,x_dot_index] = actions[a_index]\n",
    "                    \n",
    "    return(policy)                    \n",
    " \n",
    "nr.seed(233566)\n",
    "SARSA_Tile_Policy = tile_SARSA_policy(episodes = 5000)   \n",
    "SARSA_Tile_Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with actions specified for each tile in the coding scheme. This policy is not truly optimal since we are using a coarse coding and limited the number of episodes to 5,000. \n",
    "\n",
    "We can get a better felling for how this policy changes with state (position and velocity) by plotting the policy array. The color coding indicates the action given the state as parameterized with the tile coding. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy):\n",
    "    plt.imshow(policy)\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.title('Acceleration given position and velocity')\n",
    "    plt.colorbar()\n",
    "    \n",
    "display_policy(SARSA_Tile_Policy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This display shows that the policy is not really optimal. For the most part, positive acceleration is applied when the case is traveling toward the goal. However, in some cases negative acceleration is applied. Further, negative acceleration is not applied in all the cases one should expect, and there are many states for which 0 acceleration is used. \n",
    "\n",
    "The code in the cell below applies the computed policy to one episode of the mountain car problem. The car is started with a random initial velocity and the policy is followed until the goal is achieved. Charts of results are then plotted. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the car state\n",
    "a = [0.0]\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "print('Starting positon = ' + str(x[0]))\n",
    "\n",
    "## Iterate until termination\n",
    "i = 0    \n",
    "done = False\n",
    "while not done:\n",
    "# for _ in range(1000):    \n",
    "    ## Update the position and velocity    \n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a[i])\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "    ## Get the indices\n",
    "    x_index = x_state(x[i])\n",
    "    x_dot_index = x_dot_state(x_dot[i])\n",
    "        \n",
    "    ## Update acceleration using the heuristic\n",
    "    i = i + 1    \n",
    "    a.append(SARSA_Tile_Policy[x_index, x_dot_index])\n",
    "        \n",
    "plot_car_a(x,x_dot,a)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The car reaches the goal in a reasonable number of time steps, however, it is clear from these charts this is not an optimal policy. For example, notice how many time steps have no acceleration applied. \n",
    "\n",
    "If you have time you can run the above code blocks for 10,000, 20,000 or even larger numbers of episodes. You can also use a finer weight grid or overlapping weight grids. You should see an notable improvement in the policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
